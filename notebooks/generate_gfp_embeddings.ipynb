{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c1baab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing number of rows from 613866 to 523369\n"
     ]
    }
   ],
   "source": [
    "gfp_df = pd.read_csv(\"data/gfp/gfp_dataset.csv\")\n",
    "gfp_df[\"label\"] = gfp_df[\"inactive\"].astype(int)\n",
    "\n",
    "\n",
    "sub_df = gfp_df[gfp_df.num_muts <= 10]\n",
    "\n",
    "print(\"Reducing number of rows from %d to %d\" % (len(gfp_df), len(sub_df)))\n",
    "\n",
    "sub_df.to_csv(\"data/gfp_dataset_10mut.csv\", index=False)\n",
    "gfp_df = sub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python ../code/embedding_engine.py --action embed_parallel --dataset_file /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/data/gfp_dataset_10mut.csv --model esm2_t33_650M_UR50D --evaluation_path //home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m --sequence_colname full_seq --label_column_name label --job_executing_type lsf --positions_to_embed 42 44 61 65 68 69 72 94 108 112 121 145 148 150 167 181 185 203 205 220 222 224\n",
      "[INFO] Initializing PLM with current working directory: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning\n",
      "[INFO] Successfully loaded '/home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/data/gfp_dataset_10mut.csv' with column 'full_seq'.\n",
      "[INFO] Using evaluation_path: //home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m\n",
      "[INFO] Tokenizing sequences, this may take a while\n",
      "[INFO] Caching \n",
      "\t(1) esm2_t33_650M_UR50D_encoded_sequences.pt\n",
      "[INFO] Creating jobs in: //home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel\n",
      "0_500\n",
      "500_1000\n",
      "1000_1500\n",
      "1500_2000\n",
      "2000_2500\n",
      "2500_3000\n",
      "3000_3500\n",
      "3500_4000\n",
      "4000_4500\n",
      "4500_5000\n",
      "5000_5500\n",
      "5500_6000\n",
      "6000_6500\n",
      "6500_7000\n",
      "7000_7500\n",
      "7500_8000\n",
      "8000_8500\n",
      "8500_9000\n",
      "9000_9500\n",
      "9500_10000\n",
      "10000_10500\n",
      "10500_11000\n",
      "11000_11500\n",
      "11500_12000\n",
      "12000_12500\n",
      "12500_13000\n",
      "13000_13500\n",
      "13500_14000\n",
      "14000_14500\n",
      "14500_15000\n",
      "15000_15500\n",
      "15500_16000\n",
      "16000_16500\n",
      "16500_17000\n",
      "17000_17500\n",
      "17500_18000\n",
      "18000_18500\n",
      "18500_19000\n",
      "19000_19500\n",
      "19500_20000\n",
      "20000_20500\n",
      "20500_21000\n",
      "21000_21500\n",
      "21500_22000\n",
      "22000_22500\n",
      "22500_23000\n",
      "23000_23500\n",
      "23500_24000\n",
      "24000_24500\n",
      "24500_25000\n",
      "25000_25500\n",
      "25500_26000\n",
      "26000_26500\n",
      "26500_27000\n",
      "27000_27500\n",
      "27500_28000\n",
      "28000_28500\n",
      "28500_29000\n",
      "29000_29500\n",
      "29500_30000\n",
      "30000_30500\n",
      "30500_31000\n",
      "31000_31500\n",
      "31500_32000\n",
      "32000_32500\n",
      "32500_33000\n",
      "33000_33500\n",
      "33500_34000\n",
      "34000_34500\n",
      "34500_35000\n",
      "35000_35500\n",
      "35500_36000\n",
      "36000_36500\n",
      "36500_37000\n",
      "37000_37500\n",
      "37500_38000\n",
      "38000_38500\n",
      "38500_39000\n",
      "39000_39500\n",
      "39500_40000\n",
      "40000_40500\n",
      "40500_41000\n",
      "41000_41500\n",
      "41500_42000\n",
      "42000_42500\n",
      "42500_43000\n",
      "43000_43500\n",
      "43500_44000\n",
      "44000_44500\n",
      "44500_45000\n",
      "45000_45500\n",
      "45500_46000\n",
      "46000_46500\n",
      "46500_47000\n",
      "47000_47500\n",
      "47500_48000\n",
      "48000_48500\n",
      "48500_49000\n",
      "49000_49500\n",
      "49500_50000\n",
      "50000_50500\n",
      "50500_51000\n",
      "51000_51500\n",
      "51500_52000\n",
      "52000_52500\n",
      "52500_53000\n",
      "53000_53500\n",
      "53500_54000\n",
      "54000_54500\n",
      "54500_55000\n",
      "55000_55500\n",
      "55500_56000\n",
      "56000_56500\n",
      "56500_57000\n",
      "57000_57500\n",
      "57500_58000\n",
      "58000_58500\n",
      "58500_59000\n",
      "59000_59500\n",
      "59500_60000\n",
      "60000_60500\n",
      "60500_61000\n",
      "61000_61500\n",
      "61500_62000\n",
      "62000_62500\n",
      "62500_63000\n",
      "63000_63500\n",
      "63500_64000\n",
      "64000_64500\n",
      "64500_65000\n",
      "65000_65500\n",
      "65500_66000\n",
      "66000_66500\n",
      "66500_67000\n",
      "67000_67500\n",
      "67500_68000\n",
      "68000_68500\n",
      "68500_69000\n",
      "69000_69500\n",
      "69500_70000\n",
      "70000_70500\n",
      "70500_71000\n",
      "71000_71500\n",
      "71500_72000\n",
      "72000_72500\n",
      "72500_73000\n",
      "73000_73500\n",
      "73500_74000\n",
      "74000_74500\n",
      "74500_75000\n",
      "75000_75500\n",
      "75500_76000\n",
      "76000_76500\n",
      "76500_77000\n",
      "77000_77500\n",
      "77500_78000\n",
      "78000_78500\n",
      "78500_79000\n",
      "79000_79500\n",
      "79500_80000\n",
      "80000_80500\n",
      "80500_81000\n",
      "81000_81500\n",
      "81500_82000\n",
      "82000_82500\n",
      "82500_83000\n",
      "83000_83500\n",
      "83500_84000\n",
      "84000_84500\n",
      "84500_85000\n",
      "85000_85500\n",
      "85500_86000\n",
      "86000_86500\n",
      "86500_87000\n",
      "87000_87500\n",
      "87500_88000\n",
      "88000_88500\n",
      "88500_89000\n",
      "89000_89500\n",
      "89500_90000\n",
      "90000_90500\n",
      "90500_91000\n",
      "91000_91500\n",
      "91500_92000\n",
      "92000_92500\n",
      "92500_93000\n",
      "93000_93500\n",
      "93500_94000\n",
      "94000_94500\n",
      "94500_95000\n",
      "95000_95500\n",
      "95500_96000\n",
      "96000_96500\n",
      "96500_97000\n",
      "97000_97500\n",
      "97500_98000\n",
      "98000_98500\n",
      "98500_99000\n",
      "99000_99500\n",
      "99500_100000\n",
      "100000_100500\n",
      "100500_101000\n",
      "101000_101500\n",
      "101500_102000\n",
      "102000_102500\n",
      "102500_103000\n",
      "103000_103500\n",
      "103500_104000\n",
      "104000_104500\n",
      "104500_105000\n",
      "105000_105500\n",
      "105500_106000\n",
      "106000_106500\n",
      "106500_107000\n",
      "107000_107500\n",
      "107500_108000\n",
      "108000_108500\n",
      "108500_109000\n",
      "109000_109500\n",
      "109500_110000\n",
      "110000_110500\n",
      "110500_111000\n",
      "111000_111500\n",
      "111500_112000\n",
      "112000_112500\n",
      "112500_113000\n",
      "113000_113500\n",
      "113500_114000\n",
      "114000_114500\n",
      "114500_115000\n",
      "115000_115500\n",
      "115500_116000\n",
      "116000_116500\n",
      "116500_117000\n",
      "117000_117500\n",
      "117500_118000\n",
      "118000_118500\n",
      "118500_119000\n",
      "119000_119500\n",
      "119500_120000\n",
      "120000_120500\n",
      "120500_121000\n",
      "121000_121500\n",
      "121500_122000\n",
      "122000_122500\n",
      "122500_123000\n",
      "123000_123500\n",
      "123500_124000\n",
      "124000_124500\n",
      "124500_125000\n",
      "125000_125500\n",
      "125500_126000\n",
      "126000_126500\n",
      "126500_127000\n",
      "127000_127500\n",
      "127500_128000\n",
      "128000_128500\n",
      "128500_129000\n",
      "129000_129500\n",
      "129500_130000\n",
      "130000_130500\n",
      "130500_131000\n",
      "131000_131500\n",
      "131500_132000\n",
      "132000_132500\n",
      "132500_133000\n",
      "133000_133500\n",
      "133500_134000\n",
      "134000_134500\n",
      "134500_135000\n",
      "135000_135500\n",
      "135500_136000\n",
      "136000_136500\n",
      "136500_137000\n",
      "137000_137500\n",
      "137500_138000\n",
      "138000_138500\n",
      "138500_139000\n",
      "139000_139500\n",
      "139500_140000\n",
      "140000_140500\n",
      "140500_141000\n",
      "141000_141500\n",
      "141500_142000\n",
      "142000_142500\n",
      "142500_143000\n",
      "143000_143500\n",
      "143500_144000\n",
      "144000_144500\n",
      "144500_145000\n",
      "145000_145500\n",
      "145500_146000\n",
      "146000_146500\n",
      "146500_147000\n",
      "147000_147500\n",
      "147500_148000\n",
      "148000_148500\n",
      "148500_149000\n",
      "149000_149500\n",
      "149500_150000\n",
      "150000_150500\n",
      "150500_151000\n",
      "151000_151500\n",
      "151500_152000\n",
      "152000_152500\n",
      "152500_153000\n",
      "153000_153500\n",
      "153500_154000\n",
      "154000_154500\n",
      "154500_155000\n",
      "155000_155500\n",
      "155500_156000\n",
      "156000_156500\n",
      "156500_157000\n",
      "157000_157500\n",
      "157500_158000\n",
      "158000_158500\n",
      "158500_159000\n",
      "159000_159500\n",
      "159500_160000\n",
      "160000_160500\n",
      "160500_161000\n",
      "161000_161500\n",
      "161500_162000\n",
      "162000_162500\n",
      "162500_163000\n",
      "163000_163500\n",
      "163500_164000\n",
      "164000_164500\n",
      "164500_165000\n",
      "165000_165500\n",
      "165500_166000\n",
      "166000_166500\n",
      "166500_167000\n",
      "167000_167500\n",
      "167500_168000\n",
      "168000_168500\n",
      "168500_169000\n",
      "169000_169500\n",
      "169500_170000\n",
      "170000_170500\n",
      "170500_171000\n",
      "171000_171500\n",
      "171500_172000\n",
      "172000_172500\n",
      "172500_173000\n",
      "173000_173500\n",
      "173500_174000\n",
      "174000_174500\n",
      "174500_175000\n",
      "175000_175500\n",
      "175500_176000\n",
      "176000_176500\n",
      "176500_177000\n",
      "177000_177500\n",
      "177500_178000\n",
      "178000_178500\n",
      "178500_179000\n",
      "179000_179500\n",
      "179500_180000\n",
      "180000_180500\n",
      "180500_181000\n",
      "181000_181500\n",
      "181500_182000\n",
      "182000_182500\n",
      "182500_183000\n",
      "183000_183500\n",
      "183500_184000\n",
      "184000_184500\n",
      "184500_185000\n",
      "185000_185500\n",
      "185500_186000\n",
      "186000_186500\n",
      "186500_187000\n",
      "187000_187500\n",
      "187500_188000\n",
      "188000_188500\n",
      "188500_189000\n",
      "189000_189500\n",
      "189500_190000\n",
      "190000_190500\n",
      "190500_191000\n",
      "191000_191500\n",
      "191500_192000\n",
      "192000_192500\n",
      "192500_193000\n",
      "193000_193500\n",
      "193500_194000\n",
      "194000_194500\n",
      "194500_195000\n",
      "195000_195500\n",
      "195500_196000\n",
      "196000_196500\n",
      "196500_197000\n",
      "197000_197500\n",
      "197500_198000\n",
      "198000_198500\n",
      "198500_199000\n",
      "199000_199500\n",
      "199500_200000\n",
      "200000_200500\n",
      "200500_201000\n",
      "201000_201500\n",
      "201500_202000\n",
      "202000_202500\n",
      "202500_203000\n",
      "203000_203500\n",
      "203500_204000\n",
      "204000_204500\n",
      "204500_205000\n",
      "205000_205500\n",
      "205500_206000\n",
      "206000_206500\n",
      "206500_207000\n",
      "207000_207500\n",
      "207500_208000\n",
      "208000_208500\n",
      "208500_209000\n",
      "209000_209500\n",
      "209500_210000\n",
      "210000_210500\n",
      "210500_211000\n",
      "211000_211500\n",
      "211500_212000\n",
      "212000_212500\n",
      "212500_213000\n",
      "213000_213500\n",
      "213500_214000\n",
      "214000_214500\n",
      "214500_215000\n",
      "215000_215500\n",
      "215500_216000\n",
      "216000_216500\n",
      "216500_217000\n",
      "217000_217500\n",
      "217500_218000\n",
      "218000_218500\n",
      "218500_219000\n",
      "219000_219500\n",
      "219500_220000\n",
      "220000_220500\n",
      "220500_221000\n",
      "221000_221500\n",
      "221500_222000\n",
      "222000_222500\n",
      "222500_223000\n",
      "223000_223500\n",
      "223500_224000\n",
      "224000_224500\n",
      "224500_225000\n",
      "225000_225500\n",
      "225500_226000\n",
      "226000_226500\n",
      "226500_227000\n",
      "227000_227500\n",
      "227500_228000\n",
      "228000_228500\n",
      "228500_229000\n",
      "229000_229500\n",
      "229500_230000\n",
      "230000_230500\n",
      "230500_231000\n",
      "231000_231500\n",
      "231500_232000\n",
      "232000_232500\n",
      "232500_233000\n",
      "233000_233500\n",
      "233500_234000\n",
      "234000_234500\n",
      "234500_235000\n",
      "235000_235500\n",
      "235500_236000\n",
      "236000_236500\n",
      "236500_237000\n",
      "237000_237500\n",
      "237500_238000\n",
      "238000_238500\n",
      "238500_239000\n",
      "239000_239500\n",
      "239500_240000\n",
      "240000_240500\n",
      "240500_241000\n",
      "241000_241500\n",
      "241500_242000\n",
      "242000_242500\n",
      "242500_243000\n",
      "243000_243500\n",
      "243500_244000\n",
      "244000_244500\n",
      "244500_245000\n",
      "245000_245500\n",
      "245500_246000\n",
      "246000_246500\n",
      "246500_247000\n",
      "247000_247500\n",
      "247500_248000\n",
      "248000_248500\n",
      "248500_249000\n",
      "249000_249500\n",
      "249500_250000\n",
      "250000_250500\n",
      "250500_251000\n",
      "251000_251500\n",
      "251500_252000\n",
      "252000_252500\n",
      "252500_253000\n",
      "253000_253500\n",
      "253500_254000\n",
      "254000_254500\n",
      "254500_255000\n",
      "255000_255500\n",
      "255500_256000\n",
      "256000_256500\n",
      "256500_257000\n",
      "257000_257500\n",
      "257500_258000\n",
      "258000_258500\n",
      "258500_259000\n",
      "259000_259500\n",
      "259500_260000\n",
      "260000_260500\n",
      "260500_261000\n",
      "261000_261500\n",
      "261500_262000\n",
      "262000_262500\n",
      "262500_263000\n",
      "263000_263500\n",
      "263500_264000\n",
      "264000_264500\n",
      "264500_265000\n",
      "265000_265500\n",
      "265500_266000\n",
      "266000_266500\n",
      "266500_267000\n",
      "267000_267500\n",
      "267500_268000\n",
      "268000_268500\n",
      "268500_269000\n",
      "269000_269500\n",
      "269500_270000\n",
      "270000_270500\n",
      "270500_271000\n",
      "271000_271500\n",
      "271500_272000\n",
      "272000_272500\n",
      "272500_273000\n",
      "273000_273500\n",
      "273500_274000\n",
      "274000_274500\n",
      "274500_275000\n",
      "275000_275500\n",
      "275500_276000\n",
      "276000_276500\n",
      "276500_277000\n",
      "277000_277500\n",
      "277500_278000\n",
      "278000_278500\n",
      "278500_279000\n",
      "279000_279500\n",
      "279500_280000\n",
      "280000_280500\n",
      "280500_281000\n",
      "281000_281500\n",
      "281500_282000\n",
      "282000_282500\n",
      "282500_283000\n",
      "283000_283500\n",
      "283500_284000\n",
      "284000_284500\n",
      "284500_285000\n",
      "285000_285500\n",
      "285500_286000\n",
      "286000_286500\n",
      "286500_287000\n",
      "287000_287500\n",
      "287500_288000\n",
      "288000_288500\n",
      "288500_289000\n",
      "289000_289500\n",
      "289500_290000\n",
      "290000_290500\n",
      "290500_291000\n",
      "291000_291500\n",
      "291500_292000\n",
      "292000_292500\n",
      "292500_293000\n",
      "293000_293500\n",
      "293500_294000\n",
      "294000_294500\n",
      "294500_295000\n",
      "295000_295500\n",
      "295500_296000\n",
      "296000_296500\n",
      "296500_297000\n",
      "297000_297500\n",
      "297500_298000\n",
      "298000_298500\n",
      "298500_299000\n",
      "299000_299500\n",
      "299500_300000\n",
      "300000_300500\n",
      "300500_301000\n",
      "301000_301500\n",
      "301500_302000\n",
      "302000_302500\n",
      "302500_303000\n",
      "303000_303500\n",
      "303500_304000\n",
      "304000_304500\n",
      "304500_305000\n",
      "305000_305500\n",
      "305500_306000\n",
      "306000_306500\n",
      "306500_307000\n",
      "307000_307500\n",
      "307500_308000\n",
      "308000_308500\n",
      "308500_309000\n",
      "309000_309500\n",
      "309500_310000\n",
      "310000_310500\n",
      "310500_311000\n",
      "311000_311500\n",
      "311500_312000\n",
      "312000_312500\n",
      "312500_313000\n",
      "313000_313500\n",
      "313500_314000\n",
      "314000_314500\n",
      "314500_315000\n",
      "315000_315500\n",
      "315500_316000\n",
      "316000_316500\n",
      "316500_317000\n",
      "317000_317500\n",
      "317500_318000\n",
      "318000_318500\n",
      "318500_319000\n",
      "319000_319500\n",
      "319500_320000\n",
      "320000_320500\n",
      "320500_321000\n",
      "321000_321500\n",
      "321500_322000\n",
      "322000_322500\n",
      "322500_323000\n",
      "323000_323500\n",
      "323500_324000\n",
      "324000_324500\n",
      "324500_325000\n",
      "325000_325500\n",
      "325500_326000\n",
      "326000_326500\n",
      "326500_327000\n",
      "327000_327500\n",
      "327500_328000\n",
      "328000_328500\n",
      "328500_329000\n",
      "329000_329500\n",
      "329500_330000\n",
      "330000_330500\n",
      "330500_331000\n",
      "331000_331500\n",
      "331500_332000\n",
      "332000_332500\n",
      "332500_333000\n",
      "333000_333500\n",
      "333500_334000\n",
      "334000_334500\n",
      "334500_335000\n",
      "335000_335500\n",
      "335500_336000\n",
      "336000_336500\n",
      "336500_337000\n",
      "337000_337500\n",
      "337500_338000\n",
      "338000_338500\n",
      "338500_339000\n",
      "339000_339500\n",
      "339500_340000\n",
      "340000_340500\n",
      "340500_341000\n",
      "341000_341500\n",
      "341500_342000\n",
      "342000_342500\n",
      "342500_343000\n",
      "343000_343500\n",
      "343500_344000\n",
      "344000_344500\n",
      "344500_345000\n",
      "345000_345500\n",
      "345500_346000\n",
      "346000_346500\n",
      "346500_347000\n",
      "347000_347500\n",
      "347500_348000\n",
      "348000_348500\n",
      "348500_349000\n",
      "349000_349500\n",
      "349500_350000\n",
      "350000_350500\n",
      "350500_351000\n",
      "351000_351500\n",
      "351500_352000\n",
      "352000_352500\n",
      "352500_353000\n",
      "353000_353500\n",
      "353500_354000\n",
      "354000_354500\n",
      "354500_355000\n",
      "355000_355500\n",
      "355500_356000\n",
      "356000_356500\n",
      "356500_357000\n",
      "357000_357500\n",
      "357500_358000\n",
      "358000_358500\n",
      "358500_359000\n",
      "359000_359500\n",
      "359500_360000\n",
      "360000_360500\n",
      "360500_361000\n",
      "361000_361500\n",
      "361500_362000\n",
      "362000_362500\n",
      "362500_363000\n",
      "363000_363500\n",
      "363500_364000\n",
      "364000_364500\n",
      "364500_365000\n",
      "365000_365500\n",
      "365500_366000\n",
      "366000_366500\n",
      "366500_367000\n",
      "367000_367500\n",
      "367500_368000\n",
      "368000_368500\n",
      "368500_369000\n",
      "369000_369500\n",
      "369500_370000\n",
      "370000_370500\n",
      "370500_371000\n",
      "371000_371500\n",
      "371500_372000\n",
      "372000_372500\n",
      "372500_373000\n",
      "373000_373500\n",
      "373500_374000\n",
      "374000_374500\n",
      "374500_375000\n",
      "375000_375500\n",
      "375500_376000\n",
      "376000_376500\n",
      "376500_377000\n",
      "377000_377500\n",
      "377500_378000\n",
      "378000_378500\n",
      "378500_379000\n",
      "379000_379500\n",
      "379500_380000\n",
      "380000_380500\n",
      "380500_381000\n",
      "381000_381500\n",
      "381500_382000\n",
      "382000_382500\n",
      "382500_383000\n",
      "383000_383500\n",
      "383500_384000\n",
      "384000_384500\n",
      "384500_385000\n",
      "385000_385500\n",
      "385500_386000\n",
      "386000_386500\n",
      "386500_387000\n",
      "387000_387500\n",
      "387500_388000\n",
      "388000_388500\n",
      "388500_389000\n",
      "389000_389500\n",
      "389500_390000\n",
      "390000_390500\n",
      "390500_391000\n",
      "391000_391500\n",
      "391500_392000\n",
      "392000_392500\n",
      "392500_393000\n",
      "393000_393500\n",
      "393500_394000\n",
      "394000_394500\n",
      "394500_395000\n",
      "395000_395500\n",
      "395500_396000\n",
      "396000_396500\n",
      "396500_397000\n",
      "397000_397500\n",
      "397500_398000\n",
      "398000_398500\n",
      "398500_399000\n",
      "399000_399500\n",
      "399500_400000\n",
      "400000_400500\n",
      "400500_401000\n",
      "401000_401500\n",
      "401500_402000\n",
      "402000_402500\n",
      "402500_403000\n",
      "403000_403500\n",
      "403500_404000\n",
      "404000_404500\n",
      "404500_405000\n",
      "405000_405500\n",
      "405500_406000\n",
      "406000_406500\n",
      "406500_407000\n",
      "407000_407500\n",
      "407500_408000\n",
      "408000_408500\n",
      "408500_409000\n",
      "409000_409500\n",
      "409500_410000\n",
      "410000_410500\n",
      "410500_411000\n",
      "411000_411500\n",
      "411500_412000\n",
      "412000_412500\n",
      "412500_413000\n",
      "413000_413500\n",
      "413500_414000\n",
      "414000_414500\n",
      "414500_415000\n",
      "415000_415500\n",
      "415500_416000\n",
      "416000_416500\n",
      "416500_417000\n",
      "417000_417500\n",
      "417500_418000\n",
      "418000_418500\n",
      "418500_419000\n",
      "419000_419500\n",
      "419500_420000\n",
      "420000_420500\n",
      "420500_421000\n",
      "421000_421500\n",
      "421500_422000\n",
      "422000_422500\n",
      "422500_423000\n",
      "423000_423500\n",
      "423500_424000\n",
      "424000_424500\n",
      "424500_425000\n",
      "425000_425500\n",
      "425500_426000\n",
      "426000_426500\n",
      "426500_427000\n",
      "427000_427500\n",
      "427500_428000\n",
      "428000_428500\n",
      "428500_429000\n",
      "429000_429500\n",
      "429500_430000\n",
      "430000_430500\n",
      "430500_431000\n",
      "431000_431500\n",
      "431500_432000\n",
      "432000_432500\n",
      "432500_433000\n",
      "433000_433500\n",
      "433500_434000\n",
      "434000_434500\n",
      "434500_435000\n",
      "435000_435500\n",
      "435500_436000\n",
      "436000_436500\n",
      "436500_437000\n",
      "437000_437500\n",
      "437500_438000\n",
      "438000_438500\n",
      "438500_439000\n",
      "439000_439500\n",
      "439500_440000\n",
      "440000_440500\n",
      "440500_441000\n",
      "441000_441500\n",
      "441500_442000\n",
      "442000_442500\n",
      "442500_443000\n",
      "443000_443500\n",
      "443500_444000\n",
      "444000_444500\n",
      "444500_445000\n",
      "445000_445500\n",
      "445500_446000\n",
      "446000_446500\n",
      "446500_447000\n",
      "447000_447500\n",
      "447500_448000\n",
      "448000_448500\n",
      "448500_449000\n",
      "449000_449500\n",
      "449500_450000\n",
      "450000_450500\n",
      "450500_451000\n",
      "451000_451500\n",
      "451500_452000\n",
      "452000_452500\n",
      "452500_453000\n",
      "453000_453500\n",
      "453500_454000\n",
      "454000_454500\n",
      "454500_455000\n",
      "455000_455500\n",
      "455500_456000\n",
      "456000_456500\n",
      "456500_457000\n",
      "457000_457500\n",
      "457500_458000\n",
      "458000_458500\n",
      "458500_459000\n",
      "459000_459500\n",
      "459500_460000\n",
      "460000_460500\n",
      "460500_461000\n",
      "461000_461500\n",
      "461500_462000\n",
      "462000_462500\n",
      "462500_463000\n",
      "463000_463500\n",
      "463500_464000\n",
      "464000_464500\n",
      "464500_465000\n",
      "465000_465500\n",
      "465500_466000\n",
      "466000_466500\n",
      "466500_467000\n",
      "467000_467500\n",
      "467500_468000\n",
      "468000_468500\n",
      "468500_469000\n",
      "469000_469500\n",
      "469500_470000\n",
      "470000_470500\n",
      "470500_471000\n",
      "471000_471500\n",
      "471500_472000\n",
      "472000_472500\n",
      "472500_473000\n",
      "473000_473500\n",
      "473500_474000\n",
      "474000_474500\n",
      "474500_475000\n",
      "475000_475500\n",
      "475500_476000\n",
      "476000_476500\n",
      "476500_477000\n",
      "477000_477500\n",
      "477500_478000\n",
      "478000_478500\n",
      "478500_479000\n",
      "479000_479500\n",
      "479500_480000\n",
      "480000_480500\n",
      "480500_481000\n",
      "481000_481500\n",
      "481500_482000\n",
      "482000_482500\n",
      "482500_483000\n",
      "483000_483500\n",
      "483500_484000\n",
      "484000_484500\n",
      "484500_485000\n",
      "485000_485500\n",
      "485500_486000\n",
      "486000_486500\n",
      "486500_487000\n",
      "487000_487500\n",
      "487500_488000\n",
      "488000_488500\n",
      "488500_489000\n",
      "489000_489500\n",
      "489500_490000\n",
      "490000_490500\n",
      "490500_491000\n",
      "491000_491500\n",
      "491500_492000\n",
      "492000_492500\n",
      "492500_493000\n",
      "493000_493500\n",
      "493500_494000\n",
      "494000_494500\n",
      "494500_495000\n",
      "495000_495500\n",
      "495500_496000\n",
      "496000_496500\n",
      "496500_497000\n",
      "497000_497500\n",
      "497500_498000\n",
      "498000_498500\n",
      "498500_499000\n",
      "499000_499500\n",
      "499500_500000\n",
      "500000_500500\n",
      "500500_501000\n",
      "501000_501500\n",
      "501500_502000\n",
      "502000_502500\n",
      "502500_503000\n",
      "503000_503500\n",
      "503500_504000\n",
      "504000_504500\n",
      "504500_505000\n",
      "505000_505500\n",
      "505500_506000\n",
      "506000_506500\n",
      "506500_507000\n",
      "507000_507500\n",
      "507500_508000\n",
      "508000_508500\n",
      "508500_509000\n",
      "509000_509500\n",
      "509500_510000\n",
      "510000_510500\n",
      "510500_511000\n",
      "511000_511500\n",
      "511500_512000\n",
      "512000_512500\n",
      "512500_513000\n",
      "513000_513500\n",
      "513500_514000\n",
      "514000_514500\n",
      "514500_515000\n",
      "515000_515500\n",
      "515500_516000\n",
      "516000_516500\n",
      "516500_517000\n",
      "517000_517500\n",
      "517500_518000\n",
      "518000_518500\n",
      "518500_519000\n",
      "519000_519500\n",
      "519500_520000\n",
      "520000_520500\n",
      "520500_521000\n",
      "521000_521500\n",
      "521500_522000\n",
      "522000_522500\n",
      "522500_523000\n",
      "523000_523369\n",
      "[INFO] Created 1047 jobs in: //home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel\n",
      "[INFO] Registered job for dataset gfp_dataset_10mut.csv with 1047 chunks in log: all_keydatasets_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: 0 args: ['python', '../code/embedding_engine.py', '--act...>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_python_script(args, cwd=None):\n",
    "    \"\"\"\n",
    "    Executes a Python script with the given arguments list in the specified directory.\n",
    "    Prints STDOUT in real time.\n",
    "    The first element should be the script path, followed by its arguments.\n",
    "    Example: [\"other_script.py\", \"--foo\", \"bar\"]\n",
    "    \"\"\"\n",
    "    cmd = [\"python\"] + args\n",
    "    print(\"Running command:\", \" \".join(cmd))\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        cwd=cwd\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        stdout_line = process.stdout.readline()\n",
    "        if stdout_line:\n",
    "            print(stdout_line, end=\"\", flush=True)\n",
    "        elif process.poll() is not None:\n",
    "            break\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end=\"\", flush=True)\n",
    "\n",
    "    stderr = process.stderr.read()\n",
    "    if stderr:\n",
    "        print(\"\\n--- STDERR ---\\n\", stderr)\n",
    "\n",
    "    process.stdout.close()\n",
    "    process.stderr.close()\n",
    "    return process\n",
    "\n",
    "\n",
    "args = [\"../code/embedding_engine.py\",\n",
    "        \"--action\", \"embed_parallel\",\n",
    "        \"--dataset_file\", \"%s/data/gfp/gfp_dataset_10mut.csv\" % os.getcwd(),\n",
    "        #\"--n_workers\", \"5\",\n",
    "        #\"--execute_after_parallel_generation\", \"false\",\n",
    "        \"--model\", \"esm2_t33_650M_UR50D\",\n",
    "        \"--evaluation_path\", \"/%s/results/gfp_embeddings/esm_650m\" % os.getcwd(),\n",
    "        \"--sequence_colname\", \"full_seq\",\n",
    "        \"--label_column_name\", \"label\",\n",
    "        \"--job_executing_type\", \"lsf\",\n",
    "        \"--positions_to_embed\", \"42\", \"44\", \"61\", \"65\", \"68\", \"69\", \"72\", \"94\", \"108\", \"112\", \"121\", \"145\", \"148\", \"150\", \"167\", \"181\", \"185\", \"203\", \"205\", \"220\", \"222\", \"224\"]\n",
    "\n",
    "\n",
    "run_python_script(args, cwd=os.path.abspath(\"../code\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "01fa3dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python ../code/embedding_engine.py --action execute_workers --job_executing_type lsf --jobs_path /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel --n_workers 15\n",
      "[INFO] Initializing PLM with current working directory: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning\n",
      "[INFO] Created job execution library at: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118\n",
      "Submitting job 1 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_53738345 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_53738345 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_53738345.sh\n",
      "Job submitted successfully. Output: Job <755001> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 2 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_91820116 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_91820116 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_91820116.sh\n",
      "Job submitted successfully. Output: Job <755006> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 3 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_23178936 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_23178936 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_23178936.sh\n",
      "Job submitted successfully. Output: Job <755007> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 4 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_79658885 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_79658885 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_79658885.sh\n",
      "Job submitted successfully. Output: Job <755008> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 5 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_89088612 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_89088612 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_89088612.sh\n",
      "Job submitted successfully. Output: Job <755010> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 6 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_33079814 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_33079814 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_33079814.sh\n",
      "Job submitted successfully. Output: Job <755011> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 7 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_39828492 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_39828492 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_39828492.sh\n",
      "Job submitted successfully. Output: Job <755016> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 8 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_56224877 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_56224877 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_56224877.sh\n",
      "Job submitted successfully. Output: Job <755017> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 9 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_21888463 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_21888463 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_21888463.sh\n",
      "Job submitted successfully. Output: Job <755020> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 10 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_58295480 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_58295480 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_58295480.sh\n",
      "Job submitted successfully. Output: Job <755023> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 11 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_39853551 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_39853551 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_39853551.sh\n",
      "Job submitted successfully. Output: Job <755025> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 12 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_65010501 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_65010501 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_65010501.sh\n",
      "Job submitted successfully. Output: Job <755026> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 13 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_42703498 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_42703498 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_42703498.sh\n",
      "Job submitted successfully. Output: Job <755028> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 14 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_48389272 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_48389272 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_48389272.sh\n",
      "Job submitted successfully. Output: Job <755031> is submitted to queue <short-gpu>.\n",
      "\n",
      "Submitting job 15 via:  bsub -n 2 -gpu num=1:gmem=24G:aff=yes -R same[gpumodel] -R rusage[mem=64GB] -R span[ptile=2] -e /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/err_file_worker_92785769 -o /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_outputs/out_file_worker_92785769 -q short-gpu /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/job_worker_92785769.sh\n",
      "Job submitted successfully. Output: Job <755033> is submitted to queue <short-gpu>.\n",
      "\n",
      "[INFO] Wrote workers metadata to: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel/job_execution_20251211_212118/workers_metadata.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: 0 args: ['python', '../code/embedding_engine.py', '--act...>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute workers after jobs parallel generation\n",
    "\n",
    "jobs_path = \"/home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel\"\n",
    "args = [\"../code/embedding_engine.py\",\n",
    "        \"--action\", \"execute_workers\",\n",
    "        \"--job_executing_type\", \"lsf\",\n",
    "        \"--jobs_path\", \"%s\" % jobs_path,\n",
    "        \"--n_workers\", \"15\"]\n",
    "\n",
    "run_python_script(args, cwd=os.path.abspath(\"../code\"))\n",
    "\n",
    "\n",
    "# In case something fails!\n",
    "# args = [\"../code/embedding_engine.py\",\n",
    "#         \"--action\", \"execute_workers\",\n",
    "#         \"--job_executing_type\", \"lsf\",\n",
    "#         \"--jobs_path\", ##### \n",
    "#         \"--specific_job_i\", \"1046\",\n",
    "#         \"--n_workers\", \"10\"]\n",
    "\n",
    "# run_python_script(args, cwd=os.path.abspath(\"../code\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9534df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python ../code/embedding_engine.py --action check_on_jobs --jobs_path /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel\n",
      "[INFO] Initializing PLM with current working directory: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning\n",
      "[INFO] Checking on jobs in: /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m/jobs_20251211_205625_parallel\n",
      "[INFO] Args: {'average_embeddings': False, 'positions_to_embed': [42, 44, 61, 65, 68, 69, 72, 94, 108, 112, 121, 145, 148, 150, 167, 181, 185, 203, 205, 220, 222, 224], 'dataset_file': '/home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/data/gfp_dataset_10mut.csv', 'n_workers': 5, 'print_done_jobs': False, 'execute_after_parallel_generation': False, 'model': 'esm2_t33_650M_UR50D', 'evaluation_path': '//home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/results/gfp_embeddings/esm_650m', 'sequence_colname': 'full_seq', 'label_column_name': 'label', 'job_executing_type': 'lsf'}\n",
      "Total jobs listed: 1047\n",
      "Jobs done: 1047\n",
      "Jobs left: 0\n",
      "Unfinished jobs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: 0 args: ['python', '../code/embedding_engine.py', '--act...>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"../code/embedding_engine.py\",\n",
    "        \"--action\", \"check_on_jobs\",\n",
    "        \"--jobs_path\", \"%s\" % jobs_path]\n",
    "\n",
    "run_python_script(args, cwd=os.path.abspath(\"../code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3715bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
      "/tmp/ipykernel_3576768/3057819653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/gfp/gfp_dataset_10mut.csv\")\n",
    "df[\"sub_indices\"] = 0\n",
    "\n",
    "for i in range(1,11):\n",
    "    sub_indices_of_nmut = np.where(df[\"num_muts\"] == i)[0]\n",
    "    df[\"sub_indices\"].iloc[sub_indices_of_nmut] = np.arange(0,len(sub_indices_of_nmut))\n",
    "    sub_df = df.iloc[sub_indices_of_nmut]\n",
    "    sub_df.to_csv(\"data/gfp/gfp_dataset_10mut_nmut_%d.csv\" % i, index=False)\n",
    "\n",
    "df.to_csv(\"data/gfp/gfp_dataset_10mut.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//results/gfp_embeddings/esm_8m/evaluations and saving to /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2727016933.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"y_value.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"indices.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 sequences for nmut 1 in folder 349500_350000\n",
      "Analyzed 21/56 sequences\n",
      "Found 3 sequences for nmut 1 in folder 350000_350500\n",
      "Analyzed 24/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 336000_336500\n",
      "Analyzed 25/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 334000_334500\n",
      "Analyzed 26/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 521500_522000\n",
      "Analyzed 27/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 351000_351500\n",
      "Analyzed 28/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 310000_310500\n",
      "Analyzed 29/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 348000_348500\n",
      "Analyzed 30/56 sequences\n",
      "Found 5 sequences for nmut 1 in folder 349000_349500\n",
      "Analyzed 35/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 340000_340500\n",
      "Analyzed 36/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 341500_342000\n",
      "Analyzed 37/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 314500_315000\n",
      "Analyzed 38/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 136000_136500\n",
      "Analyzed 39/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 338500_339000\n",
      "Analyzed 40/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 337500_338000\n",
      "Analyzed 41/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 347000_347500\n",
      "Analyzed 42/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 352500_353000\n",
      "Analyzed 43/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 343000_343500\n",
      "Analyzed 44/56 sequences\n",
      "Found 2 sequences for nmut 1 in folder 0_500\n",
      "Analyzed 46/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 274500_275000\n",
      "Analyzed 47/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 353000_353500\n",
      "Analyzed 48/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 346000_346500\n",
      "Analyzed 49/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 351500_352000\n",
      "Analyzed 50/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 348500_349000\n",
      "Analyzed 51/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 347500_348000\n",
      "Analyzed 52/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 344500_345000\n",
      "Analyzed 53/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 195000_195500\n",
      "Analyzed 54/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 297500_298000\n",
      "Analyzed 55/56 sequences\n",
      "Found 1 sequences for nmut 1 in folder 326000_326500\n",
      "Analyzed 56/56 sequences\n",
      "####### CLEARED NMUTS = 1 for /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "####### SANITY ASSERTS PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2727016933.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"y_value.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"indices.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 sequences for nmut 2 in folder 194500_195000\n",
      "Analyzed 5/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 339000_339500\n",
      "Analyzed 6/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 114500_115000\n",
      "Analyzed 7/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 119500_120000\n",
      "Analyzed 8/711 sequences\n",
      "Found 7 sequences for nmut 2 in folder 346500_347000\n",
      "Analyzed 15/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 331000_331500\n",
      "Analyzed 16/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 101000_101500\n",
      "Analyzed 17/711 sequences\n",
      "Found 86 sequences for nmut 2 in folder 349500_350000\n",
      "Analyzed 103/711 sequences\n",
      "Found 34 sequences for nmut 2 in folder 350000_350500\n",
      "Analyzed 137/711 sequences\n",
      "Found 9 sequences for nmut 2 in folder 336000_336500\n",
      "Analyzed 146/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 263500_264000\n",
      "Analyzed 147/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 196000_196500\n",
      "Analyzed 148/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 315000_315500\n",
      "Analyzed 149/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 196500_197000\n",
      "Analyzed 150/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 131000_131500\n",
      "Analyzed 151/711 sequences\n",
      "Found 3 sequences for nmut 2 in folder 345000_345500\n",
      "Analyzed 154/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 335000_335500\n",
      "Analyzed 155/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 522500_523000\n",
      "Analyzed 157/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 339500_340000\n",
      "Analyzed 159/711 sequences\n",
      "Found 5 sequences for nmut 2 in folder 521000_521500\n",
      "Analyzed 164/711 sequences\n",
      "Found 13 sequences for nmut 2 in folder 334000_334500\n",
      "Analyzed 177/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 187500_188000\n",
      "Analyzed 178/711 sequences\n",
      "Found 13 sequences for nmut 2 in folder 521500_522000\n",
      "Analyzed 191/711 sequences\n",
      "Found 14 sequences for nmut 2 in folder 351000_351500\n",
      "Analyzed 205/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 333500_334000\n",
      "Analyzed 206/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 134000_134500\n",
      "Analyzed 207/711 sequences\n",
      "Found 4 sequences for nmut 2 in folder 350500_351000\n",
      "Analyzed 211/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 518000_518500\n",
      "Analyzed 213/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 515000_515500\n",
      "Analyzed 214/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 490500_491000\n",
      "Analyzed 215/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 519000_519500\n",
      "Analyzed 216/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 192000_192500\n",
      "Analyzed 218/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 320500_321000\n",
      "Analyzed 219/711 sequences\n",
      "Found 5 sequences for nmut 2 in folder 343500_344000\n",
      "Analyzed 224/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 193000_193500\n",
      "Analyzed 225/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 324000_324500\n",
      "Analyzed 227/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 312500_313000\n",
      "Analyzed 228/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 193500_194000\n",
      "Analyzed 229/711 sequences\n",
      "Found 8 sequences for nmut 2 in folder 325500_326000\n",
      "Analyzed 237/711 sequences\n",
      "Found 8 sequences for nmut 2 in folder 335500_336000\n",
      "Analyzed 245/711 sequences\n",
      "Found 12 sequences for nmut 2 in folder 310000_310500\n",
      "Analyzed 257/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 277000_277500\n",
      "Analyzed 258/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 76000_76500\n",
      "Analyzed 259/711 sequences\n",
      "Found 12 sequences for nmut 2 in folder 348000_348500\n",
      "Analyzed 271/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 274000_274500\n",
      "Analyzed 273/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 275000_275500\n",
      "Analyzed 274/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 191000_191500\n",
      "Analyzed 275/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 310500_311000\n",
      "Analyzed 276/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 318000_318500\n",
      "Analyzed 277/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 297000_297500\n",
      "Analyzed 278/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 255000_255500\n",
      "Analyzed 279/711 sequences\n",
      "Found 44 sequences for nmut 2 in folder 349000_349500\n",
      "Analyzed 323/711 sequences\n",
      "Found 14 sequences for nmut 2 in folder 340000_340500\n",
      "Analyzed 337/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 137500_138000\n",
      "Analyzed 338/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 296500_297000\n",
      "Analyzed 339/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 132500_133000\n",
      "Analyzed 340/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 509500_510000\n",
      "Analyzed 341/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 306000_306500\n",
      "Analyzed 342/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 311000_311500\n",
      "Analyzed 343/711 sequences\n",
      "Found 14 sequences for nmut 2 in folder 341500_342000\n",
      "Analyzed 357/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 503500_504000\n",
      "Analyzed 358/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 512500_513000\n",
      "Analyzed 359/711 sequences\n",
      "Found 3 sequences for nmut 2 in folder 344000_344500\n",
      "Analyzed 362/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 332500_333000\n",
      "Analyzed 363/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 299500_300000\n",
      "Analyzed 364/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 228000_228500\n",
      "Analyzed 365/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 327500_328000\n",
      "Analyzed 366/711 sequences\n",
      "Found 5 sequences for nmut 2 in folder 352000_352500\n",
      "Analyzed 371/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 520000_520500\n",
      "Analyzed 372/711 sequences\n",
      "Found 17 sequences for nmut 2 in folder 314500_315000\n",
      "Analyzed 389/711 sequences\n",
      "Found 11 sequences for nmut 2 in folder 136000_136500\n",
      "Analyzed 400/711 sequences\n",
      "Found 18 sequences for nmut 2 in folder 338500_339000\n",
      "Analyzed 418/711 sequences\n",
      "Found 18 sequences for nmut 2 in folder 337500_338000\n",
      "Analyzed 436/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 420000_420500\n",
      "Analyzed 437/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 329500_330000\n",
      "Analyzed 438/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 333000_333500\n",
      "Analyzed 439/711 sequences\n",
      "Found 13 sequences for nmut 2 in folder 347000_347500\n",
      "Analyzed 452/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 130500_131000\n",
      "Analyzed 453/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 341000_341500\n",
      "Analyzed 454/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 317000_317500\n",
      "Analyzed 455/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 195500_196000\n",
      "Analyzed 456/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 247500_248000\n",
      "Analyzed 457/711 sequences\n",
      "Found 15 sequences for nmut 2 in folder 352500_353000\n",
      "Analyzed 472/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 314000_314500\n",
      "Analyzed 474/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 273500_274000\n",
      "Analyzed 475/711 sequences\n",
      "Found 9 sequences for nmut 2 in folder 343000_343500\n",
      "Analyzed 484/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 298500_299000\n",
      "Analyzed 485/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 342000_342500\n",
      "Analyzed 487/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 275500_276000\n",
      "Analyzed 488/711 sequences\n",
      "Found 19 sequences for nmut 2 in folder 0_500\n",
      "Analyzed 507/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 289500_290000\n",
      "Analyzed 508/711 sequences\n",
      "Found 17 sequences for nmut 2 in folder 274500_275000\n",
      "Analyzed 525/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 326500_327000\n",
      "Analyzed 526/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 447500_448000\n",
      "Analyzed 527/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 125000_125500\n",
      "Analyzed 528/711 sequences\n",
      "Found 5 sequences for nmut 2 in folder 135500_136000\n",
      "Analyzed 533/711 sequences\n",
      "Found 14 sequences for nmut 2 in folder 353000_353500\n",
      "Analyzed 547/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 170500_171000\n",
      "Analyzed 548/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 180500_181000\n",
      "Analyzed 549/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 298000_298500\n",
      "Analyzed 551/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 135000_135500\n",
      "Analyzed 553/711 sequences\n",
      "Found 10 sequences for nmut 2 in folder 346000_346500\n",
      "Analyzed 563/711 sequences\n",
      "Found 7 sequences for nmut 2 in folder 345500_346000\n",
      "Analyzed 570/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 302000_302500\n",
      "Analyzed 571/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 325000_325500\n",
      "Analyzed 572/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 138500_139000\n",
      "Analyzed 573/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 336500_337000\n",
      "Analyzed 574/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 189500_190000\n",
      "Analyzed 575/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 323500_324000\n",
      "Analyzed 576/711 sequences\n",
      "Found 3 sequences for nmut 2 in folder 334500_335000\n",
      "Analyzed 579/711 sequences\n",
      "Found 2 sequences for nmut 2 in folder 337000_337500\n",
      "Analyzed 581/711 sequences\n",
      "Found 16 sequences for nmut 2 in folder 351500_352000\n",
      "Analyzed 597/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 313500_314000\n",
      "Analyzed 598/711 sequences\n",
      "Found 14 sequences for nmut 2 in folder 348500_349000\n",
      "Analyzed 612/711 sequences\n",
      "Found 3 sequences for nmut 2 in folder 338000_338500\n",
      "Analyzed 615/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 137000_137500\n",
      "Analyzed 616/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 324500_325000\n",
      "Analyzed 617/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 309500_310000\n",
      "Analyzed 618/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 523000_523369\n",
      "Analyzed 619/711 sequences\n",
      "Found 22 sequences for nmut 2 in folder 347500_348000\n",
      "Analyzed 641/711 sequences\n",
      "Found 11 sequences for nmut 2 in folder 344500_345000\n",
      "Analyzed 652/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 308000_308500\n",
      "Analyzed 653/711 sequences\n",
      "Found 13 sequences for nmut 2 in folder 195000_195500\n",
      "Analyzed 666/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 340500_341000\n",
      "Analyzed 667/711 sequences\n",
      "Found 16 sequences for nmut 2 in folder 297500_298000\n",
      "Analyzed 683/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 322000_322500\n",
      "Analyzed 684/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 342500_343000\n",
      "Analyzed 685/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 272000_272500\n",
      "Analyzed 686/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 517000_517500\n",
      "Analyzed 687/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 268500_269000\n",
      "Analyzed 688/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 319000_319500\n",
      "Analyzed 689/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 282500_283000\n",
      "Analyzed 690/711 sequences\n",
      "Found 19 sequences for nmut 2 in folder 326000_326500\n",
      "Analyzed 709/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 128500_129000\n",
      "Analyzed 710/711 sequences\n",
      "Found 1 sequences for nmut 2 in folder 294000_294500\n",
      "Analyzed 711/711 sequences\n",
      "####### CLEARED NMUTS = 1 for /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "####### SANITY ASSERTS PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2727016933.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"y_value.pt\"))\n",
      "/tmp/ipykernel_3576768/2727016933.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"indices.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 sequences for nmut 3 in folder 516500_517000\n",
      "Analyzed 1/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 48000_48500\n",
      "Analyzed 2/3733 sequences\n",
      "Found 42 sequences for nmut 3 in folder 194500_195000\n",
      "Analyzed 44/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 339000_339500\n",
      "Analyzed 57/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 114500_115000\n",
      "Analyzed 67/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 119500_120000\n",
      "Analyzed 81/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 261000_261500\n",
      "Analyzed 82/3733 sequences\n",
      "Found 50 sequences for nmut 3 in folder 346500_347000\n",
      "Analyzed 132/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 331000_331500\n",
      "Analyzed 145/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 330000_330500\n",
      "Analyzed 146/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 511000_511500\n",
      "Analyzed 147/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 101000_101500\n",
      "Analyzed 162/3733 sequences\n",
      "Found 169 sequences for nmut 3 in folder 349500_350000\n",
      "Analyzed 331/3733 sequences\n",
      "Found 99 sequences for nmut 3 in folder 350000_350500\n",
      "Analyzed 430/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 316000_316500\n",
      "Analyzed 431/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 287500_288000\n",
      "Analyzed 432/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 328000_328500\n",
      "Analyzed 434/3733 sequences\n",
      "Found 30 sequences for nmut 3 in folder 336000_336500\n",
      "Analyzed 464/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 327000_327500\n",
      "Analyzed 465/3733 sequences\n",
      "Found 3 sequences for nmut 3 in folder 119000_119500\n",
      "Analyzed 468/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 263500_264000\n",
      "Analyzed 482/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 180000_180500\n",
      "Analyzed 484/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 130000_130500\n",
      "Analyzed 486/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 196000_196500\n",
      "Analyzed 499/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 254500_255000\n",
      "Analyzed 501/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 315000_315500\n",
      "Analyzed 506/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 99500_100000\n",
      "Analyzed 507/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 196500_197000\n",
      "Analyzed 511/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 131000_131500\n",
      "Analyzed 518/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 29500_30000\n",
      "Analyzed 519/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 388500_389000\n",
      "Analyzed 520/3733 sequences\n",
      "Found 18 sequences for nmut 3 in folder 345000_345500\n",
      "Analyzed 538/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 335000_335500\n",
      "Analyzed 553/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 133000_133500\n",
      "Analyzed 555/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 522500_523000\n",
      "Analyzed 571/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 179000_179500\n",
      "Analyzed 572/3733 sequences\n",
      "Found 25 sequences for nmut 3 in folder 339500_340000\n",
      "Analyzed 597/3733 sequences\n",
      "Found 41 sequences for nmut 3 in folder 521000_521500\n",
      "Analyzed 638/3733 sequences\n",
      "Found 48 sequences for nmut 3 in folder 334000_334500\n",
      "Analyzed 686/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 418500_419000\n",
      "Analyzed 687/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 282000_282500\n",
      "Analyzed 693/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 129000_129500\n",
      "Analyzed 694/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 187500_188000\n",
      "Analyzed 704/3733 sequences\n",
      "Found 59 sequences for nmut 3 in folder 521500_522000\n",
      "Analyzed 763/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 294500_295000\n",
      "Analyzed 764/3733 sequences\n",
      "Found 61 sequences for nmut 3 in folder 351000_351500\n",
      "Analyzed 825/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 225500_226000\n",
      "Analyzed 826/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 419000_419500\n",
      "Analyzed 827/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 333500_334000\n",
      "Analyzed 841/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 129500_130000\n",
      "Analyzed 842/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 134000_134500\n",
      "Analyzed 852/3733 sequences\n",
      "Found 35 sequences for nmut 3 in folder 350500_351000\n",
      "Analyzed 887/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 118500_119000\n",
      "Analyzed 888/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 229000_229500\n",
      "Analyzed 889/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 235500_236000\n",
      "Analyzed 890/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 307500_308000\n",
      "Analyzed 891/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 489500_490000\n",
      "Analyzed 892/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 518000_518500\n",
      "Analyzed 904/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 318500_319000\n",
      "Analyzed 905/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 416000_416500\n",
      "Analyzed 906/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 490000_490500\n",
      "Analyzed 907/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 512000_512500\n",
      "Analyzed 911/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 515000_515500\n",
      "Analyzed 916/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 182500_183000\n",
      "Analyzed 917/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 127000_127500\n",
      "Analyzed 918/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 75500_76000\n",
      "Analyzed 920/3733 sequences\n",
      "Found 3 sequences for nmut 3 in folder 273000_273500\n",
      "Analyzed 923/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 490500_491000\n",
      "Analyzed 936/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 305500_306000\n",
      "Analyzed 937/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 519500_520000\n",
      "Analyzed 942/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 518500_519000\n",
      "Analyzed 944/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 519000_519500\n",
      "Analyzed 954/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 192000_192500\n",
      "Analyzed 964/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 320500_321000\n",
      "Analyzed 977/3733 sequences\n",
      "Found 3 sequences for nmut 3 in folder 138000_138500\n",
      "Analyzed 980/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 184000_184500\n",
      "Analyzed 981/3733 sequences\n",
      "Found 37 sequences for nmut 3 in folder 343500_344000\n",
      "Analyzed 1018/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 193000_193500\n",
      "Analyzed 1029/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 324000_324500\n",
      "Analyzed 1040/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 407000_407500\n",
      "Analyzed 1041/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 290000_290500\n",
      "Analyzed 1042/3733 sequences\n",
      "Found 9 sequences for nmut 3 in folder 312500_313000\n",
      "Analyzed 1051/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 215000_215500\n",
      "Analyzed 1052/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 122500_123000\n",
      "Analyzed 1053/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 193500_194000\n",
      "Analyzed 1068/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 503000_503500\n",
      "Analyzed 1069/3733 sequences\n",
      "Found 40 sequences for nmut 3 in folder 325500_326000\n",
      "Analyzed 1109/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 103000_103500\n",
      "Analyzed 1110/3733 sequences\n",
      "Found 38 sequences for nmut 3 in folder 335500_336000\n",
      "Analyzed 1148/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 151500_152000\n",
      "Analyzed 1149/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 307000_307500\n",
      "Analyzed 1150/3733 sequences\n",
      "Found 30 sequences for nmut 3 in folder 310000_310500\n",
      "Analyzed 1180/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 277000_277500\n",
      "Analyzed 1194/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 256500_257000\n",
      "Analyzed 1195/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 110000_110500\n",
      "Analyzed 1196/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 76000_76500\n",
      "Analyzed 1212/3733 sequences\n",
      "Found 57 sequences for nmut 3 in folder 348000_348500\n",
      "Analyzed 1269/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 274000_274500\n",
      "Analyzed 1280/3733 sequences\n",
      "Found 9 sequences for nmut 3 in folder 275000_275500\n",
      "Analyzed 1289/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 191000_191500\n",
      "Analyzed 1297/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 310500_311000\n",
      "Analyzed 1308/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 316500_317000\n",
      "Analyzed 1310/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 491000_491500\n",
      "Analyzed 1311/3733 sequences\n",
      "Found 18 sequences for nmut 3 in folder 318000_318500\n",
      "Analyzed 1329/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 297000_297500\n",
      "Analyzed 1342/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 101500_102000\n",
      "Analyzed 1343/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 417500_418000\n",
      "Analyzed 1344/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 255000_255500\n",
      "Analyzed 1360/3733 sequences\n",
      "Found 126 sequences for nmut 3 in folder 349000_349500\n",
      "Analyzed 1486/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 84500_85000\n",
      "Analyzed 1487/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 311500_312000\n",
      "Analyzed 1488/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 254000_254500\n",
      "Analyzed 1489/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 266000_266500\n",
      "Analyzed 1490/3733 sequences\n",
      "Found 63 sequences for nmut 3 in folder 340000_340500\n",
      "Analyzed 1553/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 293500_294000\n",
      "Analyzed 1554/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 522000_522500\n",
      "Analyzed 1559/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 112500_113000\n",
      "Analyzed 1560/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 137500_138000\n",
      "Analyzed 1570/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 296500_297000\n",
      "Analyzed 1583/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 73500_74000\n",
      "Analyzed 1584/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 132500_133000\n",
      "Analyzed 1597/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 328500_329000\n",
      "Analyzed 1598/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 315500_316000\n",
      "Analyzed 1602/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 308500_309000\n",
      "Analyzed 1603/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 159000_159500\n",
      "Analyzed 1604/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 465000_465500\n",
      "Analyzed 1605/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 447000_447500\n",
      "Analyzed 1612/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 509500_510000\n",
      "Analyzed 1618/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 306000_306500\n",
      "Analyzed 1630/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 329000_329500\n",
      "Analyzed 1637/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 171000_171500\n",
      "Analyzed 1638/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 311000_311500\n",
      "Analyzed 1648/3733 sequences\n",
      "Found 61 sequences for nmut 3 in folder 341500_342000\n",
      "Analyzed 1709/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 484500_485000\n",
      "Analyzed 1710/3733 sequences\n",
      "Found 9 sequences for nmut 3 in folder 185500_186000\n",
      "Analyzed 1719/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 503500_504000\n",
      "Analyzed 1725/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 312000_312500\n",
      "Analyzed 1733/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 514500_515000\n",
      "Analyzed 1734/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 446000_446500\n",
      "Analyzed 1735/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 115500_116000\n",
      "Analyzed 1736/3733 sequences\n",
      "Found 3 sequences for nmut 3 in folder 512500_513000\n",
      "Analyzed 1739/3733 sequences\n",
      "Found 27 sequences for nmut 3 in folder 344000_344500\n",
      "Analyzed 1766/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 283500_284000\n",
      "Analyzed 1767/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 269000_269500\n",
      "Analyzed 1769/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 70500_71000\n",
      "Analyzed 1770/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 504000_504500\n",
      "Analyzed 1771/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 293000_293500\n",
      "Analyzed 1772/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 332500_333000\n",
      "Analyzed 1786/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 281000_281500\n",
      "Analyzed 1787/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 134500_135000\n",
      "Analyzed 1795/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 401000_401500\n",
      "Analyzed 1796/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 114000_114500\n",
      "Analyzed 1797/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 299500_300000\n",
      "Analyzed 1807/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 413000_413500\n",
      "Analyzed 1808/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 191500_192000\n",
      "Analyzed 1814/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 190500_191000\n",
      "Analyzed 1816/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 227000_227500\n",
      "Analyzed 1817/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 249000_249500\n",
      "Analyzed 1818/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 133500_134000\n",
      "Analyzed 1824/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 228000_228500\n",
      "Analyzed 1837/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 279000_279500\n",
      "Analyzed 1838/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 327500_328000\n",
      "Analyzed 1853/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 410000_410500\n",
      "Analyzed 1854/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 301500_302000\n",
      "Analyzed 1856/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 321500_322000\n",
      "Analyzed 1857/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 299000_299500\n",
      "Analyzed 1861/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 487500_488000\n",
      "Analyzed 1862/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 204000_204500\n",
      "Analyzed 1863/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 352000_352500\n",
      "Analyzed 1878/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 520000_520500\n",
      "Analyzed 1889/3733 sequences\n",
      "Found 49 sequences for nmut 3 in folder 314500_315000\n",
      "Analyzed 1938/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 244500_245000\n",
      "Analyzed 1939/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 125500_126000\n",
      "Analyzed 1940/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 306500_307000\n",
      "Analyzed 1941/3733 sequences\n",
      "Found 42 sequences for nmut 3 in folder 136000_136500\n",
      "Analyzed 1983/3733 sequences\n",
      "Found 55 sequences for nmut 3 in folder 338500_339000\n",
      "Analyzed 2038/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 475000_475500\n",
      "Analyzed 2039/3733 sequences\n",
      "Found 69 sequences for nmut 3 in folder 337500_338000\n",
      "Analyzed 2108/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 420000_420500\n",
      "Analyzed 2121/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 329500_330000\n",
      "Analyzed 2128/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 333000_333500\n",
      "Analyzed 2142/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 177000_177500\n",
      "Analyzed 2143/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 331500_332000\n",
      "Analyzed 2144/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 305000_305500\n",
      "Analyzed 2145/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 321000_321500\n",
      "Analyzed 2146/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 271500_272000\n",
      "Analyzed 2151/3733 sequences\n",
      "Found 52 sequences for nmut 3 in folder 347000_347500\n",
      "Analyzed 2203/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 194000_194500\n",
      "Analyzed 2210/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 130500_131000\n",
      "Analyzed 2221/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 341000_341500\n",
      "Analyzed 2234/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 317000_317500\n",
      "Analyzed 2246/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 195500_196000\n",
      "Analyzed 2258/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 292000_292500\n",
      "Analyzed 2259/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 190000_190500\n",
      "Analyzed 2260/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 304000_304500\n",
      "Analyzed 2261/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 247500_248000\n",
      "Analyzed 2269/3733 sequences\n",
      "Found 55 sequences for nmut 3 in folder 352500_353000\n",
      "Analyzed 2324/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 248000_248500\n",
      "Analyzed 2325/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 314000_314500\n",
      "Analyzed 2341/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 76500_77000\n",
      "Analyzed 2342/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 302500_303000\n",
      "Analyzed 2343/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 273500_274000\n",
      "Analyzed 2357/3733 sequences\n",
      "Found 40 sequences for nmut 3 in folder 343000_343500\n",
      "Analyzed 2397/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 298500_299000\n",
      "Analyzed 2404/3733 sequences\n",
      "Found 23 sequences for nmut 3 in folder 342000_342500\n",
      "Analyzed 2427/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 275500_276000\n",
      "Analyzed 2442/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 231000_231500\n",
      "Analyzed 2443/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 309000_309500\n",
      "Analyzed 2448/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 65000_65500\n",
      "Analyzed 2449/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 332000_332500\n",
      "Analyzed 2451/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 419500_420000\n",
      "Analyzed 2455/3733 sequences\n",
      "Found 56 sequences for nmut 3 in folder 0_500\n",
      "Analyzed 2511/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 289500_290000\n",
      "Analyzed 2524/3733 sequences\n",
      "Found 61 sequences for nmut 3 in folder 274500_275000\n",
      "Analyzed 2585/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 56000_56500\n",
      "Analyzed 2586/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 517500_518000\n",
      "Analyzed 2592/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 421000_421500\n",
      "Analyzed 2594/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 300500_301000\n",
      "Analyzed 2595/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 181000_181500\n",
      "Analyzed 2596/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 326500_327000\n",
      "Analyzed 2612/3733 sequences\n",
      "Found 10 sequences for nmut 3 in folder 447500_448000\n",
      "Analyzed 2622/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 322500_323000\n",
      "Analyzed 2624/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 125000_125500\n",
      "Analyzed 2630/3733 sequences\n",
      "Found 46 sequences for nmut 3 in folder 135500_136000\n",
      "Analyzed 2676/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 317500_318000\n",
      "Analyzed 2678/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 241000_241500\n",
      "Analyzed 2679/3733 sequences\n",
      "Found 41 sequences for nmut 3 in folder 353000_353500\n",
      "Analyzed 2720/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 170000_170500\n",
      "Analyzed 2721/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 247000_247500\n",
      "Analyzed 2722/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 264500_265000\n",
      "Analyzed 2723/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 170500_171000\n",
      "Analyzed 2731/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 92000_92500\n",
      "Analyzed 2732/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 180500_181000\n",
      "Analyzed 2736/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 298000_298500\n",
      "Analyzed 2752/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 132000_132500\n",
      "Analyzed 2756/3733 sequences\n",
      "Found 23 sequences for nmut 3 in folder 135000_135500\n",
      "Analyzed 2779/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 270000_270500\n",
      "Analyzed 2780/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 281500_282000\n",
      "Analyzed 2781/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 514000_514500\n",
      "Analyzed 2782/3733 sequences\n",
      "Found 39 sequences for nmut 3 in folder 346000_346500\n",
      "Analyzed 2821/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 516000_516500\n",
      "Analyzed 2822/3733 sequences\n",
      "Found 50 sequences for nmut 3 in folder 345500_346000\n",
      "Analyzed 2872/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 302000_302500\n",
      "Analyzed 2883/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 113500_114000\n",
      "Analyzed 2884/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 325000_325500\n",
      "Analyzed 2897/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 138500_139000\n",
      "Analyzed 2905/3733 sequences\n",
      "Found 9 sequences for nmut 3 in folder 336500_337000\n",
      "Analyzed 2914/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 499000_499500\n",
      "Analyzed 2915/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 189500_190000\n",
      "Analyzed 2920/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 330500_331000\n",
      "Analyzed 2922/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 323500_324000\n",
      "Analyzed 2936/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 290500_291000\n",
      "Analyzed 2937/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 189000_189500\n",
      "Analyzed 2938/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 271000_271500\n",
      "Analyzed 2940/3733 sequences\n",
      "Found 26 sequences for nmut 3 in folder 334500_335000\n",
      "Analyzed 2966/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 448000_448500\n",
      "Analyzed 2967/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 169500_170000\n",
      "Analyzed 2968/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 337000_337500\n",
      "Analyzed 2984/3733 sequences\n",
      "Found 59 sequences for nmut 3 in folder 351500_352000\n",
      "Analyzed 3043/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 295500_296000\n",
      "Analyzed 3044/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 319500_320000\n",
      "Analyzed 3046/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 313500_314000\n",
      "Analyzed 3061/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 166000_166500\n",
      "Analyzed 3062/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 507500_508000\n",
      "Analyzed 3063/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 168000_168500\n",
      "Analyzed 3064/3733 sequences\n",
      "Found 69 sequences for nmut 3 in folder 348500_349000\n",
      "Analyzed 3133/3733 sequences\n",
      "Found 19 sequences for nmut 3 in folder 338000_338500\n",
      "Analyzed 3152/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 286000_286500\n",
      "Analyzed 3153/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 137000_137500\n",
      "Analyzed 3166/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 262500_263000\n",
      "Analyzed 3167/3733 sequences\n",
      "Found 14 sequences for nmut 3 in folder 324500_325000\n",
      "Analyzed 3181/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 505500_506000\n",
      "Analyzed 3182/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 77000_77500\n",
      "Analyzed 3183/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 309500_310000\n",
      "Analyzed 3195/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 523000_523369\n",
      "Analyzed 3202/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 267500_268000\n",
      "Analyzed 3203/3733 sequences\n",
      "Found 95 sequences for nmut 3 in folder 347500_348000\n",
      "Analyzed 3298/3733 sequences\n",
      "Found 56 sequences for nmut 3 in folder 344500_345000\n",
      "Analyzed 3354/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 320000_320500\n",
      "Analyzed 3356/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 492000_492500\n",
      "Analyzed 3357/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 162000_162500\n",
      "Analyzed 3358/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 515500_516000\n",
      "Analyzed 3359/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 96500_97000\n",
      "Analyzed 3360/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 479500_480000\n",
      "Analyzed 3361/3733 sequences\n",
      "Found 12 sequences for nmut 3 in folder 308000_308500\n",
      "Analyzed 3373/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 75000_75500\n",
      "Analyzed 3374/3733 sequences\n",
      "Found 55 sequences for nmut 3 in folder 195000_195500\n",
      "Analyzed 3429/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 188500_189000\n",
      "Analyzed 3430/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 259000_259500\n",
      "Analyzed 3431/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 228500_229000\n",
      "Analyzed 3433/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 421500_422000\n",
      "Analyzed 3434/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 502500_503000\n",
      "Analyzed 3435/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 495000_495500\n",
      "Analyzed 3436/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 340500_341000\n",
      "Analyzed 3449/3733 sequences\n",
      "Found 52 sequences for nmut 3 in folder 297500_298000\n",
      "Analyzed 3501/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 323000_323500\n",
      "Analyzed 3502/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 123500_124000\n",
      "Analyzed 3503/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 251000_251500\n",
      "Analyzed 3504/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 445000_445500\n",
      "Analyzed 3505/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 106000_106500\n",
      "Analyzed 3506/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 416500_417000\n",
      "Analyzed 3507/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 187000_187500\n",
      "Analyzed 3508/3733 sequences\n",
      "Found 4 sequences for nmut 3 in folder 136500_137000\n",
      "Analyzed 3512/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 322000_322500\n",
      "Analyzed 3525/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 342500_343000\n",
      "Analyzed 3538/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 227500_228000\n",
      "Analyzed 3539/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 272000_272500\n",
      "Analyzed 3552/3733 sequences\n",
      "Found 8 sequences for nmut 3 in folder 520500_521000\n",
      "Analyzed 3560/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 263000_263500\n",
      "Analyzed 3561/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 173500_174000\n",
      "Analyzed 3562/3733 sequences\n",
      "Found 6 sequences for nmut 3 in folder 517000_517500\n",
      "Analyzed 3568/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 120500_121000\n",
      "Analyzed 3569/3733 sequences\n",
      "Found 15 sequences for nmut 3 in folder 268500_269000\n",
      "Analyzed 3584/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 446500_447000\n",
      "Analyzed 3585/3733 sequences\n",
      "Found 16 sequences for nmut 3 in folder 319000_319500\n",
      "Analyzed 3601/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 186500_187000\n",
      "Analyzed 3602/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 128000_128500\n",
      "Analyzed 3603/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 296000_296500\n",
      "Analyzed 3608/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 127500_128000\n",
      "Analyzed 3609/3733 sequences\n",
      "Found 11 sequences for nmut 3 in folder 282500_283000\n",
      "Analyzed 3620/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 253500_254000\n",
      "Analyzed 3621/3733 sequences\n",
      "Found 2 sequences for nmut 3 in folder 100500_101000\n",
      "Analyzed 3623/3733 sequences\n",
      "Found 73 sequences for nmut 3 in folder 326000_326500\n",
      "Analyzed 3696/3733 sequences\n",
      "Found 7 sequences for nmut 3 in folder 128500_129000\n",
      "Analyzed 3703/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 117000_117500\n",
      "Analyzed 3704/3733 sequences\n",
      "Found 13 sequences for nmut 3 in folder 294000_294500\n",
      "Analyzed 3717/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 272500_273000\n",
      "Analyzed 3718/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 313000_313500\n",
      "Analyzed 3719/3733 sequences\n",
      "Found 3 sequences for nmut 3 in folder 192500_193000\n",
      "Analyzed 3722/3733 sequences\n",
      "Found 5 sequences for nmut 3 in folder 276500_277000\n",
      "Analyzed 3727/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 78500_79000\n",
      "Analyzed 3728/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 268000_268500\n",
      "Analyzed 3729/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 124500_125000\n",
      "Analyzed 3730/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 289000_289500\n",
      "Analyzed 3731/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 222000_222500\n",
      "Analyzed 3732/3733 sequences\n",
      "Found 1 sequences for nmut 3 in folder 246000_246500\n",
      "Analyzed 3733/3733 sequences\n",
      "####### CLEARED NMUTS = 1 for /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "####### SANITY ASSERTS PASSED\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/\"\n",
    "intermediate_result_paths = [#\"%s/results/gfp_embeddings/esm_650m/evaluations\" % base_path,\n",
    "                    #\"%s/results/gfp_embeddings/esm_35m/evaluations\" % base_path,\n",
    "                    \"%s/results/gfp_embeddings/esm_8m/evaluations\" % base_path]\n",
    "\n",
    "final_save_path = [#\"%s/results/gfp_embeddings/esm_650m\" % base_path,\n",
    "                #\"%s/results/gfp_embeddings/esm_35m\" % base_path,\n",
    "                \"%s/data/gfp/embeddings/esm_8m\" % base_path]\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/gfp_dataset_10mut.csv\")\n",
    "for intermediate_result_path, final_save_path in zip(intermediate_result_paths, final_save_path):\n",
    "\n",
    "    print(\"Processing %s and saving to %s\" % (intermediate_result_path, final_save_path))\n",
    "\n",
    "    subfolders = [f.name for f in os.scandir(intermediate_result_path) if f.is_dir()]\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        final_embeddings = None\n",
    "        final_labels = None\n",
    "        final_indices = None\n",
    "\n",
    "        sub_df = pd.read_csv(\"%s/data/gfp_dataset_10mut_nmut_%d.csv\" % (base_path, i))\n",
    "        \n",
    "        sequences_analysed = 0 \n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
    "            labels = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"y_value.pt\"))\n",
    "            indices = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"indices.pt\"))\n",
    "\n",
    "            if final_embeddings is None:\n",
    "                final_embeddings = torch.zeros([sub_df.shape[0], embeddings.shape[1], embeddings.shape[2]])\n",
    "                final_labels = torch.zeros([sub_df.shape[0]])\n",
    "                final_indices = torch.zeros([sub_df.shape[0]], dtype=torch.int64)\n",
    "\n",
    "            relevant_indices_wthin_intermediate_result = df.iloc[indices][\"num_muts\"] == i\n",
    "            within_sub_df_indices = df.iloc[indices][relevant_indices_wthin_intermediate_result][\"sub_indices\"]\n",
    "            \n",
    "            assert (sum(relevant_indices_wthin_intermediate_result) == len(within_sub_df_indices))\n",
    "            \n",
    "            if len(within_sub_df_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            sequences_analysed += len(within_sub_df_indices)\n",
    "            print(\"Found %d sequences for nmut %d in folder %s\" % (len(within_sub_df_indices), i, subfolder))\n",
    "            print(\"Analyzed %d/%d sequences\" % (sequences_analysed, sub_df.shape[0]))\n",
    "            \n",
    "\n",
    "            final_embeddings[within_sub_df_indices.to_numpy()] = embeddings[relevant_indices_wthin_intermediate_result.to_numpy()]\n",
    "            final_labels[within_sub_df_indices.to_numpy()] = labels[relevant_indices_wthin_intermediate_result.to_numpy()].to(torch.float)\n",
    "            final_indices[within_sub_df_indices.to_numpy()] = indices[relevant_indices_wthin_intermediate_result.to_numpy()]\n",
    "\n",
    "            if sequences_analysed == sub_df.shape[0]:\n",
    "                break\n",
    "\n",
    "        torch.save(final_embeddings, os.path.join(final_save_path, \"embeddings_of_nmut_%d.pt\" % i))\n",
    "        torch.save(final_labels, os.path.join(final_save_path, \"y_values_of_nmut_%d.pt\" % i))\n",
    "        torch.save(final_indices, os.path.join(final_save_path, \"indices_of_nmut_%d.pt\" % i))\n",
    "        \n",
    "        print(\"####### CLEARED NMUTS = %d for %s\" % (i, final_save_path))\n",
    "        assert(sum(sub_df[\"label\"] == final_labels.to(torch.int).numpy()) == sub_df.shape[0])\n",
    "        assert(sum(final_indices == np.where(df[\"num_muts\"] == i)[0]) == sub_df.shape[0])\n",
    "        print(\"####### SANITY ASSERTS PASSED\")\n",
    "    break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f30585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"data/gfp_dataset_10mut.csv\")\n",
    "\n",
    "base_path = \"/home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks/\"\n",
    "intermediate_result_paths = [\"%s/results/gfp_embeddings/esm_650m/evaluations\" % base_path,\n",
    "                            \"%s/results/gfp_embeddings/esm_35m/evaluations\" % base_path,\n",
    "                            \"%s/results/gfp_embeddings/esm_8m/evaluations\" % base_path]\n",
    "\n",
    "final_save_path = [\"%s/data/gfp/embeddings/esm_650m\" % base_path,\n",
    "                    \"%s/data/gfp/embeddings/esm_35m\" % base_path,\n",
    "                    \"%s/data/gfp/embeddings/esm_8m\" % base_path]\n",
    "\n",
    "hidden_dim_size = [#1280, \n",
    "480, 320]\n",
    "\n",
    "df = pd.read_csv(\"data/gfp_dataset_10mut.csv\")\n",
    "\n",
    "for intermediate_result_path, final_save_path, hs in zip(intermediate_result_paths, final_save_path, hidden_dim_size):\n",
    "\n",
    "    print(\"Processing %s and saving to %s\" % (intermediate_result_path, final_save_path))\n",
    "\n",
    "    subfolders = [f.name for f in os.scandir(intermediate_result_path) if f.is_dir()]\n",
    "\n",
    "    embedding_all = torch.zeros([df.shape[0], 22, hs], dtype=torch.float)\n",
    "    label_all = torch.zeros([df.shape[0]], dtype=torch.float)\n",
    "    indices_all = torch.zeros([df.shape[0]], dtype=torch.int64)\n",
    "\n",
    "    for i, subfolder in enumerate(subfolders):\n",
    "\n",
    "        print(\"\\tLoading %s [%d/%d]\" % (subfolder, i, len(subfolders)))\n",
    "        embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
    "        embeddings = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"embeddings.pt\"))\n",
    "        labels = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"y_value.pt\"))\n",
    "        indices = torch.load(os.path.join(intermediate_result_path, subfolder, \"train\", \"indices.pt\"))\n",
    "        \n",
    "        indices_all[indices] = indices\n",
    "        label_all[indices] = labels.to(torch.float)\n",
    "        embedding_all[indices] = embeddings\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        slice_indices = np.where(df[\"num_muts\"] == i)[0]\n",
    "        print(\"\\tSaving %s [%d/%d]\" % (os.path.join(final_save_path, \"embeddings_of_nmut_%d.pt\" % i), i, 10))\n",
    "        torch.save(embedding_all[slice_indices], os.path.join(final_save_path, \"embeddings_of_nmut_%d.pt\" % i))\n",
    "        print(\"\\tSaving %s [%d/%d]\" % (os.path.join(final_save_path, \"y_values_of_nmut_%d.pt\" % i), i, 10))\n",
    "        torch.save(label_all[slice_indices], os.path.join(final_save_path, \"y_values_of_nmut_%d.pt\" % i))\n",
    "        print(\"\\tSaving %s [%d/%d]\" % (os.path.join(final_save_path, \"indices_of_nmut_%d.pt\" % i), i, 10))\n",
    "        torch.save(indices_all[slice_indices], os.path.join(final_save_path, \"indices_of_nmut_%d.pt\" % i))\n",
    "    \n",
    "    del embedding_all, label_all, indices_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 1 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 1 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 1 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 2 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 2 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 2 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 3 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 3 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 3 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 4 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 4 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 4 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 5 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 5 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 5 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 6 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 6 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 6 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 7 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 7 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 7 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 8 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 8 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 8 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 9 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 9 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 9 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576768/2171715667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
      "/tmp/ipykernel_3576768/2171715667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 10 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_650m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 10 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_35m\n",
      "Assert 1: 1, Assert 2: 1, Assert 3: 1, Assert 4: 1, Assert 5: 1\n",
      "Sanity assertions passed for 10 on /home/labs/fleishman/itayta/new_fitness_repo/fitness_learning/notebooks//data/embeddings/esm_8m\n"
     ]
    }
   ],
   "source": [
    "#ASSERTIONS AND VALIDATIONS \n",
    "final_save_path = [\"%s/data/gfp/embeddings/esm_650m\" % base_path,\n",
    "                    \"%s/data/gfp/embeddings/esm_35m\" % base_path,\n",
    "                    \"%s/data/gfp/embeddings/esm_8m\" % base_path]\n",
    "\n",
    "\n",
    "for i in range(1, 11):\n",
    "    sub_df = pd.read_csv(\"%s/data/gfp_dataset_10mut_nmut_%d.csv\" % (base_path, i))\n",
    "\n",
    "    for final_result_path in final_save_path:\n",
    "        labels = torch.load(os.path.join(final_result_path, \"y_values_of_nmut_%d.pt\" % i))\n",
    "        indices = torch.load(os.path.join(final_result_path, \"indices_of_nmut_%d.pt\" % i))\n",
    "\n",
    "\n",
    "        assert_1 = sum(sub_df[\"label\"].to_numpy() == labels.numpy()) == sub_df.shape[0]\n",
    "        assert_2 = sum(df.iloc[indices][\"label\"].to_numpy() == labels.numpy()) == sub_df.shape[0]\n",
    "        assert_3 = sum(df.iloc[indices][\"sub_indices\"].to_numpy() == sub_df[\"sub_indices\"].to_numpy()) == sub_df.shape[0]\n",
    "        assert_4 = sum(np.where(df[\"num_muts\"] == i)[0] == indices.numpy()) == sub_df.shape[0]\n",
    "        assert_5 = sum(df.iloc[np.where(df[\"num_muts\"] == i)[0]][\"label\"].to_numpy() == labels.numpy()) == sub_df.shape[0]\n",
    "        \n",
    "        assert(assert_1)\n",
    "        assert(assert_2)\n",
    "        assert(assert_3)\n",
    "        assert(assert_4)\n",
    "        assert(assert_5)\n",
    "\n",
    "        print(\"Assert 1: %d, Assert 2: %d, Assert 3: %d, Assert 4: %d, Assert 5: %d\" % (assert_1, assert_2, assert_3, assert_4, assert_5))\n",
    "        print(\"Sanity assertions passed for %d on %s\" % (i, final_result_path))\n",
    "        \n",
    "        #assert(sum(labels == df.iloc[indices][\"label\"]) == len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
